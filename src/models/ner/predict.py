import torch
import unicodedata
from transformers import AutoModelForTokenClassification, AutoTokenizer

from configuration import config


class Predictor:
    def __init__(self, model, tokenizer, device):
        self.model = model.to(device)
        self.model.eval()
        self.tokenizer = tokenizer
        self.device = device

    def _preprocess_text(self, text: str) -> str:
        '''ç”¨æˆ·è¾“å…¥æ–‡æœ¬é¢„å¤„ç†:å…¨è§’è½¬åŠè§’+å»é™¤æ‰€æœ‰ç©ºæ ¼'''
        # 1.å…¨è§’è½¬åŠè§’
        processed = []
        for char in text:
            normalized_char = unicodedata.normalize('NFKC', char)
            # å¤„ç†å…¨è§’ç¬¦å·(é™¤ç©ºæ ¼å¤–)
            if '\uFF01' <= normalized_char <= '\uFF5E':
                processed.append(chr(ord(normalized_char) - 0xfee0))
            # å¤„ç†å…¨è§’ç©ºæ ¼
            elif normalized_char == '\u3000':
                processed.append(' ')
            else:
                processed.append(normalized_char)
        text = ''.join(processed)

        # 2.å»é™¤æ‰€æœ‰ç©ºæ ¼(åŒ…æ‹¬åŠè§’ç©ºæ ¼)
        text = text.replace(' ', '')
        return text

    def predict(self, inputs: str | list):
        is_str = isinstance(inputs, str)
        inputs = [inputs] if is_str else inputs

        #ğŸ”¥å¯¹æ¯ä¸ªè¾“å…¥å…ˆé¢„å¤„ç† è½¬åŠè§’+åˆ é™¤ç©ºæ ¼
        processed_inputs = [self._preprocess_text(text) for text in inputs]

        tokens_list = [list(text.lower()) for text in processed_inputs] #â—è½¬å°å†™
        inputs_tensor = self.tokenizer(tokens_list, is_split_into_words=True, padding=True, truncation=True,
                                       return_tensors='pt')
        inputs_tensor = {k: v.to(self.device) for k, v in inputs_tensor.items()}
        with torch.no_grad():
            outputs = self.model(**inputs_tensor)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=-1).tolist()

        final_predictions = []
        for tokens, prediction in zip(tokens_list, predictions):
            prediction = prediction[1:1 + len(tokens)]
            final_prediction = [self.model.config.id2label[id] for id in prediction]
            final_predictions.append(final_prediction)

        if is_str:
            return processed_inputs[0],final_predictions[0]
        return processed_inputs,final_predictions

    def extract(self, inputs: str | list):
        is_str = isinstance(inputs, str)
        if is_str:
            inputs = [inputs]

        processed_inputs,final_predictions = self.predict(inputs)
        entities = []
        for text, final_prediction in zip(processed_inputs, final_predictions):
            # import pdb;pdb.set_trace()
            entity = self._extract_entity(text, final_prediction)
            entities.append(entity)
        if is_str:
            return entities[0]
        return entities

    def _extract_entity(self, text, final_prediction): #å•ä¸ªæ ·æœ¬ å’Œ å¯¹åº”çš„é¢„æµ‹æ ‡ç­¾
        all_entities = []
        cur_entity = ''
        for char, tag in zip(list(text), final_prediction):
            if tag == 'B':
                if cur_entity:
                    all_entities.append(cur_entity)
                    cur_entity = ''
                cur_entity += char

            elif tag == 'I':
                if cur_entity:
                    cur_entity += char

            else:
                if cur_entity:
                    all_entities.append(cur_entity)
                cur_entity = ''

        if cur_entity:
            all_entities.append(cur_entity)
        return all_entities




if __name__ == '__main__':
    model = AutoModelForTokenClassification.from_pretrained(config.CHECKPOINT_DIR / 'ner' / 'best_model')
    tokenizer = AutoTokenizer.from_pretrained(config.CHECKPOINT_DIR / 'ner' / 'best_model')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    predictor = Predictor(model=model, tokenizer=tokenizer, device=device)


    texts = 'è®²è§£ApacheSparkåˆ†å¸ƒå¼è®¡ç®— æ¡†æ¶çš„ ä½¿ç”¨,åŒ…æ‹¬RDDã€DataFrameã€SQLæŸ¥è¯¢ã€æµå¼å¤„ç†ã€ä»»åŠ¡è°ƒåº¦ç­‰ã€‚'
    texts2 = ['138_å°šç¡…è°·_Hadoop_Yarn_å®¹é‡è°ƒåº¦å™¨ä»»åŠ¡ä¼˜å…ˆçº§','è®²è§£ApacheSparkåˆ†å¸ƒå¼è®¡ç®— æ¡†æ¶çš„ ä½¿ç”¨,åŒ…æ‹¬RDDã€DataFrameã€SQLæŸ¥è¯¢ã€æµå¼å¤„ç†ã€ä»»åŠ¡è°ƒåº¦ç­‰ã€‚']
    texts3 ='<p><strong>ä»¥ä¸‹å…³äºæ–¹æ³•è°ƒç”¨çš„ä»£ç çš„æ‰§è¡Œç»“æœæ˜¯</strong></p><p>&nbsp;</p><p>&nbsp;public class Test {</p><p>&nbsp;&nbsp;&nbsp;public static void main(String args[]) {</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;int i = 99;</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mb_operate(i);</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;System.out.print(i + 100);</p><p>&nbsp;&nbsp;&nbsp;}</p><p>&nbsp;&nbsp;&nbsp;static int mb_operate(int i) {</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;i + 100;</p><p>&nbsp;&nbsp;&nbsp;}</p><p>}</p>'
    texts4 = 'ä»¥ä¸‹å…³äºæ–¹æ³•è°ƒç”¨çš„ä»£ç çš„æ‰§è¡Œç»“æœæ˜¯public class Test '
    texts5 = 'question_txt: "<p>åœ¨è¿›è¡Œ Java ä»£ç ç¼–è¯‘å’Œè¿è¡Œçš„è¿‡ç¨‹ä¸­ï¼Œå“ªç§æ ¼å¼çš„æ–‡ä»¶å¯ä»¥è¢«ç¼–è¯‘æˆå­—èŠ‚ç æ–‡ä»¶ï¼ˆï¼‰</p>"'
    # processed_texts,labels = predictor.predict(texts2)
    # for text, label in zip(processed_texts,labels):
    #     # import pdb;pdb.set_trace()
    #     print(text,'â†’',label)

    #----------------------------------------------------------
    print(predictor.extract(texts))
    #ğŸ’¡['ApacheSpark', 'RDD', 'DataFrame', 'SQLæŸ¥è¯¢']
    print(predictor.extract(texts2))
    #ğŸ’¡[['Hadoop', 'Yarn', 'å®¹é‡', 'è°ƒåº¦å™¨', 'ä»»åŠ¡', 'ä¼˜å…ˆçº§'], ['ApacheSpark', 'RDD', 'DataFrame', 'SQLæŸ¥è¯¢']]
    print(predictor.extract(texts3))
    print(predictor.extract(texts5))